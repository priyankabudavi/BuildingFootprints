{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hurricanes_buildingdetect_train_model.py",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1oy5uwqppSYSKqQ34vg0ZsR1R_VyK_JVB",
      "authorship_tag": "ABX9TyM1AOD7Bx5BMsWYpGdSBmLA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyankabudavi/BuildingFootprints/blob/main/hurricanes_buildingdetect_train_model_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8i2P2qq7-d3"
      },
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "\n",
        "##calcs\n",
        "import tensorflow as tf #numerical operations on gpu\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import tensorflow.keras.backend as K\n",
        "# from sklearn.metrics import confusion_matrix #compute confusion matrix from vectors of observed and estimated labels\n",
        "\n",
        "from collections import OrderedDict\n",
        "from random import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0LoaDI79JGA",
        "outputId": "396720ff-b34a-463c-ada3-ce0864cbae06"
      },
      "source": [
        "\n",
        "SEED=42\n",
        "np.random.seed(SEED)\n",
        "AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "MAX_EPOCHS = 200\n",
        "\n",
        "nclasses=4 #1\n",
        "\n",
        "\n",
        "# # ## scratch learning rate curve\n",
        "start_lr = 1e-07\n",
        "min_lr = start_lr\n",
        "max_lr = 1e-04\n",
        "rampup_epochs = 15\n",
        "sustain_epochs = 5\n",
        "exp_decay = .8\n",
        "\n",
        "\n",
        "# ## transfer learning rate curve\n",
        "# start_lr = 1e-06\n",
        "# min_lr = start_lr\n",
        "# max_lr = 1e-04\n",
        "# rampup_epochs = 10\n",
        "# sustain_epochs = 5\n",
        "# exp_decay = .8\n",
        "\n",
        "\n",
        "\n",
        "###############################################################\n",
        "### MODEL FUNCTIONS\n",
        "###############################################################\n",
        "#----------------------------------------------\n",
        "def get_inference_model(threshold, model): #, num_classes):\n",
        "    \"\"\"\n",
        "    get_inference_model(threshold, model)\n",
        "    This function creates an inference model consisting of an input layer for an image\n",
        "    the model predictions, decoded detections, then finally a mapping from image to detections\n",
        "    In effect it is a model nested in another model\n",
        "    INPUTS:\n",
        "        * threshold [float], the detecton probability beyond which we are confident of\n",
        "        * model [keras model], trained object detection model\n",
        "    OPTIONAL INPUTS: None\n",
        "    GLOBAL INPUTS: start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay\n",
        "    OUTPUTS:  keras model for detections on images\n",
        "    \"\"\"\n",
        "    # ANY size input\n",
        "    image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
        "\n",
        "    predictions = model(image, training=False)\n",
        "\n",
        "    detections = DecodePredictions(confidence_threshold=threshold)(image, predictions)\n",
        "\n",
        "    inference_model = tf.keras.Model(inputs=image, outputs=detections)\n",
        "    return inference_model\n",
        "\n",
        "\n",
        "def visualize_detections(\n",
        "    image, boxes, classes, scores, counter, str_prefix, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
        "):\n",
        "    \"\"\"\n",
        "    visualize_detections(image, boxes, classes, scores, counter, str_prefix, figsize=(7, 7), linewidth=1, color=[0, 0, 1])\n",
        "    This function allows for visualization of imagery and bounding boxes\n",
        "\n",
        "    INPUTS:\n",
        "        * images [ndarray]: batch of images\n",
        "        * boxes [ndarray]: batch of bounding boxes per image\n",
        "        * classes [list]: class strings\n",
        "        * scores [list]: prediction scores\n",
        "        * str_prefix [string]: filename prefix\n",
        "    OPTIONAL INPUTS:\n",
        "      * figsize=(7, 7)\n",
        "      * linewidth=1\n",
        "      * color=[0, 0, 1]\n",
        "    OUTPUTS:\n",
        "        * val_dataset [tensorflow dataset]: validation dataset\n",
        "        * train_dataset [tensorflow dataset]: training dataset\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    fig =plt.figure(figsize=figsize)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "    for box, _cls, score in zip(boxes, classes, scores):\n",
        "        text = \"{}: {:.2f}\".format(_cls, score)\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        patch = plt.Rectangle(\n",
        "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
        "        )\n",
        "        ax.add_patch(patch)\n",
        "        ax.text(\n",
        "            x1,\n",
        "            y1,\n",
        "            text,\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "    plt.savefig(str_prefix+str(counter)+'.png', dpi=200, bbox_inches='tight')\n",
        "    plt.close('all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version:  2.7.0\n",
            "Eager mode:  True\n",
            "GPU name:  []\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeYsFnMt9RnL"
      },
      "source": [
        "\n",
        "#-----------------------------------\n",
        "def file2tensor(f):\n",
        "    \"\"\"\n",
        "    file2tensor(f)\n",
        "    This function reads a jpeg image from file into a cropped and resized tensor,\n",
        "    for use in prediction with a trained mobilenet or vgg model\n",
        "    (the imagery is standardized depedning on target model framework)\n",
        "    INPUTS:\n",
        "        * f [string] file name of jpeg\n",
        "    OPTIONAL INPUTS:\n",
        "        * model = {'mobilenet' | 'vgg'}\n",
        "    OUTPUTS:\n",
        "        * image [tensor array]: unstandardized image\n",
        "        * im [tensor array]: standardized image\n",
        "    GLOBAL INPUTS: TARGET_SIZE\n",
        "    \"\"\"\n",
        "    bits = tf.io.read_file(f)\n",
        "    image = tf.image.decode_png(bits)\n",
        "\n",
        "    return image\n",
        "\n",
        "def lrfn(epoch):\n",
        "    \"\"\"\n",
        "    lrfn(epoch)\n",
        "    This function creates a custom piecewise linear-exponential learning rate function\n",
        "    for a custom learning rate scheduler. It is linear to a max, then exponentially decays\n",
        "    INPUTS: current epoch number\n",
        "    OPTIONAL INPUTS: None\n",
        "    GLOBAL INPUTS: start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay\n",
        "    OUTPUTS:  the function lr with all arguments passed\n",
        "    \"\"\"\n",
        "    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
        "        if epoch < rampup_epochs:\n",
        "            lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "        elif epoch < rampup_epochs + sustain_epochs:\n",
        "            lr = max_lr\n",
        "        else:\n",
        "            lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "        return lr\n",
        "    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n",
        "\n",
        "def prepare_image(image):\n",
        "    \"\"\"\n",
        "    prepare_image(image)\n",
        "    \"\"\n",
        "    This function resizes and pads an image, and rescales for resnet\n",
        "    INPUTS:\n",
        "        * image [tensor array]\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * image [tensor array]\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "    image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
        "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
        "    return tf.expand_dims(image, axis=0), ratio\n",
        "\n",
        "\n",
        "def compute_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    compute_iou(boxes1, boxes2)\n",
        "    This function computes pairwise IOU matrix for given two sets of boxes\n",
        "    INPUTS:\n",
        "        * boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
        "          where each box is of the format `[x, y, width, height]`.\n",
        "        * boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
        "          where each box is of the format `[x, y, width, height]`.\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        *  pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
        "           jth column holds the IOU between ith box and jth box from\n",
        "           boxes1 and boxes2 respectively.\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "    boxes1_corners = convert_to_corners(boxes1)\n",
        "    boxes2_corners = convert_to_corners(boxes2)\n",
        "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
        "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
        "    intersection = tf.maximum(0.0, rd - lu)\n",
        "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
        "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
        "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
        "    union_area = tf.maximum(\n",
        "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
        "    )\n",
        "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n",
        "\n",
        "\n",
        "# ## Implementing Anchor generator\n",
        "# Anchor boxes are fixed sized boxes that the model uses to predict the bounding\n",
        "# box for an object. It does this by regressing the offset between the location\n",
        "# of the object's center and the center of an anchor box, and then uses the width\n",
        "# and height of the anchor box to predict a relative scale of the object. In the\n",
        "# case of RetinaNet, each location on a given feature map has nine anchor boxes\n",
        "# (at three scales and three ratios).\n",
        "\n",
        "class AnchorBox:\n",
        "    \"\"\"\n",
        "    \"AnchorBox\"\n",
        "    ## Code from https://keras.io/examples/vision/retinanet/\n",
        "    Generates anchor boxes.\n",
        "    This class has operations to generate anchor boxes for feature maps at\n",
        "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
        "    format `[x, y, width, height]`.\n",
        "    INPUTS:\n",
        "      * aspect_ratios: A list of float values representing the aspect ratios of\n",
        "        the anchor boxes at each location on the feature map\n",
        "      * scales: A list of float values representing the scale of the anchor boxes\n",
        "        at each location on the feature map.\n",
        "      * num_anchors: The number of anchor boxes at each location on feature map\n",
        "      * areas: A list of float values representing the areas of the anchor\n",
        "        boxes for each feature map in the feature pyramid.\n",
        "      * strides: A list of float value representing the strides for each feature\n",
        "        map in the feature pyramid.\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * anchor boxes for all the feature maps, stacked as a single tensor with shape\n",
        "        `(total_anchors, 4)`, when AnchorBox._get_anchors() is called\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
        "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
        "\n",
        "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
        "        self._strides = [2 ** i for i in range(3, 8)]\n",
        "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
        "        self._anchor_dims = self._compute_dims()\n",
        "\n",
        "    def _compute_dims(self):\n",
        "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
        "        of the feature pyramid.\n",
        "        \"\"\"\n",
        "        anchor_dims_all = []\n",
        "        for area in self._areas:\n",
        "            anchor_dims = []\n",
        "            for ratio in self.aspect_ratios:\n",
        "                anchor_height = tf.math.sqrt(area / ratio)\n",
        "                anchor_width = area / anchor_height\n",
        "                dims = tf.reshape(\n",
        "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
        "                )\n",
        "                for scale in self.scales:\n",
        "                    anchor_dims.append(scale * dims)\n",
        "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
        "        return anchor_dims_all\n",
        "\n",
        "    def _get_anchors(self, feature_height, feature_width, level):\n",
        "        \"\"\"\n",
        "        \"_get_anchors\"\n",
        "        ## Code from https://keras.io/examples/vision/retinanet/\n",
        "        Generates anchor boxes for a given feature map size and level\n",
        "        Arguments:\n",
        "          feature_height: An integer representing the height of the feature map.\n",
        "          feature_width: An integer representing the width of the feature map.\n",
        "          level: An integer representing the level of the feature map in the\n",
        "            feature pyramid.\n",
        "        Returns:\n",
        "          anchor boxes with the shape\n",
        "          `(feature_height * feature_width * num_anchors, 4)`\n",
        "        \"\"\"\n",
        "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
        "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
        "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
        "        centers = tf.expand_dims(centers, axis=-2)\n",
        "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
        "        dims = tf.tile(\n",
        "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
        "        )\n",
        "        anchors = tf.concat([centers, dims], axis=-1)\n",
        "        return tf.reshape(\n",
        "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
        "        )\n",
        "\n",
        "    def get_anchors(self, image_height, image_width):\n",
        "        \"\"\"\n",
        "        \"get_anchors\"\n",
        "        ## Code from https://keras.io/examples/vision/retinanet/\n",
        "        Generates anchor boxes for all the feature maps of the feature pyramid.\n",
        "        Arguments:\n",
        "          image_height: Height of the input image.\n",
        "          image_width: Width of the input image.\n",
        "        Returns:\n",
        "          anchor boxes for all the feature maps, stacked as a single tensor\n",
        "            with shape `(total_anchors, 4)`\n",
        "        \"\"\"\n",
        "        anchors = [\n",
        "            self._get_anchors(\n",
        "                tf.math.ceil(image_height / 2 ** i),\n",
        "                tf.math.ceil(image_width / 2 ** i),\n",
        "                i,\n",
        "            )\n",
        "            for i in range(3, 8)\n",
        "        ]\n",
        "        return tf.concat(anchors, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "def get_backbone(noise_stdev=0.1):\n",
        "    \"\"\"\n",
        "    get_backbone()\n",
        "    ## Code from https://keras.io/examples/vision/retinanet/\n",
        "    \"\"\n",
        "    This function Builds ResNet50 with pre-trained imagenet weights\n",
        "    INPUTS: None\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * keras Model\n",
        "    GLOBAL INPUTS: BATCH_SIZE\n",
        "    \"\"\"\n",
        "    backbone = tf.keras.applications.ResNet50(\n",
        "        include_top=False, input_shape=[None, None, 3]\n",
        "    )\n",
        "    c3_output, c4_output, c5_output = [\n",
        "        backbone.get_layer(layer_name).output\n",
        "        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
        "    ]\n",
        "    # return tf.keras.Model(\n",
        "    #     inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
        "    # )\n",
        "    return tf.keras.Model(\n",
        "        inputs=[backbone.inputs], outputs=[tf.keras.layers.GaussianNoise(noise_stdev)(c3_output), tf.keras.layers.GaussianNoise(noise_stdev)(c4_output), tf.keras.layers.GaussianNoise(noise_stdev)(c5_output)]\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7udWNDxy9XW6"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "## Building Feature Pyramid Network as a custom layer\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class FeaturePyramid(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    \"FeaturePyramid\"\n",
        "    ## Code from https://keras.io/examples/vision/retinanet/\n",
        "    This class builds the Feature Pyramid with the feature maps from the backbone.\n",
        "    INPUTS:\n",
        "      * num_classes: Number of classes in the dataset.\n",
        "      * backbone: The backbone to build the feature pyramid from. Currently supports ResNet50 only (the output of get_backbone())\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * the 5-feature pyramids (feature maps) at strides `[8, 16, 32, 64, 128]`\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, noise_stdev=0.1, backbone=None, **kwargs):\n",
        "        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n",
        "        self.backbone = backbone if backbone else get_backbone(noise_stdev)\n",
        "        self.conv_c3_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c4_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c5_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c3_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c4_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c5_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c6_3x3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.conv_c7_3x3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.upsample_2x = tf.keras.layers.UpSampling2D(2)\n",
        "\n",
        "    def call(self, images, training=False):\n",
        "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
        "        p3_output = self.conv_c3_1x1(c3_output)\n",
        "        p4_output = self.conv_c4_1x1(c4_output)\n",
        "        p5_output = self.conv_c5_1x1(c5_output)\n",
        "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
        "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
        "        p3_output = self.conv_c3_3x3(p3_output)\n",
        "        p4_output = self.conv_c4_3x3(p4_output)\n",
        "        p5_output = self.conv_c5_3x3(p5_output)\n",
        "        p6_output = self.conv_c6_3x3(c5_output)\n",
        "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
        "        return p3_output, p4_output, p5_output, p6_output, p7_output\n",
        "\n",
        "\n",
        "def build_head(output_filters, bias_init):\n",
        "    \"\"\"\n",
        "    \"build_head(output_filters, bias_init)\"\n",
        "    ## Code from https://keras.io/examples/vision/retinanet/\n",
        "    This function builds the class/box predictions head.\n",
        "    INPUTS:\n",
        "        * output_filters: Number of convolution filters in the final layer.\n",
        "        * bias_init: Bias Initializer for the final convolution layer.\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * a keras sequential model representing either the classification\n",
        "          or the box regression head depending on `output_filters`.\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "    head = tf.keras.Sequential([tf.keras.Input(shape=[None, None, 256])])\n",
        "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
        "    for _ in range(4):\n",
        "        head.add(\n",
        "            tf.keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n",
        "        )\n",
        "        head.add(tf.keras.layers.ReLU())\n",
        "    head.add(\n",
        "        tf.keras.layers.Conv2D(\n",
        "            output_filters,\n",
        "            3,\n",
        "            1,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=kernel_init,\n",
        "            bias_initializer=bias_init,\n",
        "        )\n",
        "    )\n",
        "    return head\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Building RetinaNet using a subclassed model\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class RetinaNet(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    \"RetinaNet\"\n",
        "    ## Code from https://keras.io/examples/vision/retinanet/\n",
        "    This class returns a subclassed Keras model implementing the RetinaNet architecture.\n",
        "    INPUTS:\n",
        "        * num_classes: Number of classes in the dataset.\n",
        "        * backbone: The backbone to build the feature pyramid from. Supports ResNet50 only.\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * val_dataset [tensorflow dataset]: validation dataset\n",
        "        * train_dataset [tensorflow dataset]: training dataset\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, noise_stdev, backbone=None, **kwargs):\n",
        "        super(RetinaNet, self).__init__(name=\"RetinaNet\", **kwargs)\n",
        "        self.fpn = FeaturePyramid(noise_stdev, backbone)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
        "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
        "        self.box_head = build_head(9 * 4, \"zeros\")\n",
        "\n",
        "    def call(self, image, training=False):\n",
        "        features = self.fpn(image, training=training)\n",
        "        N = tf.shape(image)[0]\n",
        "        cls_outputs = []\n",
        "        box_outputs = []\n",
        "        for feature in features:\n",
        "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
        "            cls_outputs.append(\n",
        "                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
        "            )\n",
        "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
        "        box_outputs = tf.concat(box_outputs, axis=1)\n",
        "        return tf.concat([box_outputs, cls_outputs], axis=-1)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Implementing a custom layer to decode predictions\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class DecodePredictions(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    \"DecodePredictions\"\n",
        "    ## Code from https://keras.io/examples/vision/retinanet/\n",
        "    This class creates a Keras layer that decodes predictions of the RetinaNet model.\n",
        "    INPUTS:\n",
        "        * num_classes: Number of classes in the dataset\n",
        "        * confidence_threshold: Minimum class probability, below which detections\n",
        "          are pruned.\n",
        "        * nms_iou_threshold: IOU threshold for the NMS operation\n",
        "        * max_detections_per_class: Maximum number of detections to retain per class.\n",
        "        * max_detections: Maximum number of detections to retain across all classes.\n",
        "        * box_variance: The scaling factors used to scale the bounding box predictions.\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * a keras layer to decode predictions\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=1,\n",
        "        confidence_threshold=0.05,\n",
        "        nms_iou_threshold=0.5,\n",
        "        max_detections_per_class=100,\n",
        "        max_detections=100,\n",
        "        box_variance=[0.1, 0.1, 0.2, 0.2],\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(DecodePredictions, self).__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.nms_iou_threshold = nms_iou_threshold\n",
        "        self.max_detections_per_class = max_detections_per_class\n",
        "        self.max_detections = max_detections\n",
        "\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
        "        boxes = box_predictions * self._box_variance\n",
        "        boxes = tf.concat(\n",
        "            [\n",
        "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
        "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        boxes_transformed = convert_to_corners(boxes)\n",
        "        return boxes_transformed\n",
        "\n",
        "    def call(self, images, predictions):\n",
        "        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        box_predictions = predictions[:, :, :4]\n",
        "        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n",
        "        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
        "\n",
        "        return tf.image.combined_non_max_suppression(\n",
        "            tf.expand_dims(boxes, axis=2),\n",
        "            cls_predictions,\n",
        "            self.max_detections_per_class,\n",
        "            self.max_detections,\n",
        "            self.nms_iou_threshold,\n",
        "            self.confidence_threshold,\n",
        "            clip_boxes=False,\n",
        "        )\n",
        "\n",
        "\n",
        "###############################################################\n",
        "## MODEL TRAINING\n",
        "###############################################################\n",
        "\n",
        "class RetinaNetBoxLoss(tf.losses.Loss):\n",
        "    \"\"\"\n",
        "    \"RetinaNetBoxLoss\"\n",
        "    ## Code from https://keras.io/examples/vision/retinanet/\n",
        "    This class implements smooth L1 loss\n",
        "    INPUTS:\n",
        "        * y_true [tensor]: label observations\n",
        "        * y_pred [tensor]: label estimates\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * loss [tensor]\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, delta):\n",
        "        super(RetinaNetBoxLoss, self).__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
        "        )\n",
        "        self._delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        difference = y_true - y_pred\n",
        "        absolute_difference = tf.abs(difference)\n",
        "        squared_difference = difference ** 2\n",
        "        loss = tf.where(\n",
        "            tf.less(absolute_difference, self._delta),\n",
        "            0.5 * squared_difference,\n",
        "            absolute_difference - 0.5,\n",
        "        )\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
        "    \"\"\"\n",
        "    \"RetinaNetClassificationLoss\"\n",
        "    ## Code from https://keras.io/examples/vision/retinanet/\n",
        "    This class implements Focal loss\n",
        "    INPUTS:\n",
        "        * y_true [tensor]: label observations\n",
        "        * y_pred [tensor]: label estimates\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * loss [tensor]\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha, gamma):\n",
        "        super(RetinaNetClassificationLoss, self).__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
        "        )\n",
        "        self._alpha = alpha\n",
        "        self._gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            labels=y_true, logits=y_pred\n",
        "        )\n",
        "        probs = tf.nn.sigmoid(y_pred)\n",
        "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
        "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
        "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetLoss(tf.losses.Loss):\n",
        "    \"\"\"\n",
        "    \"RetinaNetLoss\"\n",
        "    ## Code from https://keras.io/examples/vision/retinanet/\n",
        "    This class is a wrapper to sum RetinaNetClassificationLoss and RetinaNetClassificationLoss outputs\n",
        "    INPUTS:\n",
        "        * y_true [tensor]: label observations\n",
        "        * y_pred [tensor]: label estimates\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * loss [tensor]\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=1, alpha=0.25, gamma=2.0, delta=1.0):\n",
        "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
        "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
        "        self._box_loss = RetinaNetBoxLoss(delta)\n",
        "        self._num_classes = num_classes\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        box_labels = y_true[:, :, :4]\n",
        "        box_predictions = y_pred[:, :, :4]\n",
        "        cls_labels = tf.one_hot(\n",
        "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
        "            depth=self._num_classes,\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "        cls_predictions = y_pred[:, :, 4:]\n",
        "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
        "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
        "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
        "        box_loss = self._box_loss(box_labels, box_predictions)\n",
        "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
        "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
        "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
        "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
        "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
        "        loss = clf_loss + box_loss\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "class LabelEncoderCoco:\n",
        "    \"\"\"\n",
        "    LabelEncoderCoco()\n",
        "    Transforms the raw labels into targets for training.\n",
        "    This class has operations to generate targets for a batch of samples which\n",
        "    is made up of the input images, bounding boxes for the objects present and\n",
        "    their class ids.\n",
        "    Attributes:\n",
        "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
        "      box_variance: The scaling factors used to scale the bounding box targets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _match_anchor_boxes(\n",
        "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
        "    ):\n",
        "        \"\"\"Matches ground truth boxes to anchor boxes based on IOU.\n",
        "        1. Calculates the pairwise IOU for the M `anchor_boxes` and N `gt_boxes`\n",
        "          to get a `(M, N)` shaped matrix.\n",
        "        2. The ground truth box with the maximum IOU in each row is assigned to\n",
        "          the anchor box provided the IOU is greater than `match_iou`.\n",
        "        3. If the maximum IOU in a row is less than `ignore_iou`, the anchor\n",
        "          box is assigned with the background class.\n",
        "        4. The remaining anchor boxes that do not have any class assigned are\n",
        "          ignored during training.\n",
        "        Arguments:\n",
        "          anchor_boxes: A float tensor with the shape `(total_anchors, 4)`\n",
        "            representing all the anchor boxes for a given input image shape,\n",
        "            where each anchor box is of the format `[x, y, width, height]`.\n",
        "          gt_boxes: A float tensor with shape `(num_objects, 4)` representing\n",
        "            the ground truth boxes, where each box is of the format\n",
        "            `[x, y, width, height]`.\n",
        "          match_iou: A float value representing the minimum IOU threshold for\n",
        "            determining if a ground truth box can be assigned to an anchor box.\n",
        "          ignore_iou: A float value representing the IOU threshold under which\n",
        "            an anchor box is assigned to the background class.\n",
        "        Returns:\n",
        "          matched_gt_idx: Index of the matched object\n",
        "          positive_mask: A mask for anchor boxes that have been assigned ground\n",
        "            truth boxes.\n",
        "          ignore_mask: A mask for anchor boxes that need to by ignored during\n",
        "            training\n",
        "        \"\"\"\n",
        "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
        "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
        "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
        "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
        "        negative_mask = tf.less(max_iou, ignore_iou)\n",
        "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
        "        return (\n",
        "            matched_gt_idx,\n",
        "            tf.cast(positive_mask, dtype=tf.float32),\n",
        "            tf.cast(ignore_mask, dtype=tf.float32),\n",
        "        )\n",
        "\n",
        "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
        "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
        "        box_target = tf.concat(\n",
        "            [\n",
        "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
        "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        box_target = box_target / self._box_variance\n",
        "        return box_target\n",
        "\n",
        "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
        "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
        "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
        "            anchor_boxes, gt_boxes\n",
        "        )\n",
        "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
        "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
        "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
        "        cls_target = tf.where(\n",
        "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
        "        )\n",
        "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
        "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
        "        label = tf.concat([box_target, cls_target], axis=-1)\n",
        "        return label\n",
        "\n",
        "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
        "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
        "        images_shape = tf.shape(batch_images)\n",
        "        batch_size = images_shape[0]\n",
        "\n",
        "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
        "        for i in range(batch_size):\n",
        "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
        "            labels = labels.write(i, label)\n",
        "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
        "        return batch_images, labels.stack()\n",
        "\n",
        "def convert_to_corners(boxes):\n",
        "    \"\"\"\n",
        "    convert_to_corners(boxes)\n",
        "    Changes the box format to corner coordinates\n",
        "    Arguments:\n",
        "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
        "        representing bounding boxes where each box is of the format\n",
        "        `[x, y, width, height]`.\n",
        "    Returns:\n",
        "      converted boxes with shape same as that of boxes.\n",
        "    \"\"\"\n",
        "    return tf.concat(\n",
        "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
        "        axis=-1,\n",
        "    )\n",
        "\n",
        "\n",
        "#----------------------------------------------\n",
        "def prepare_secoora_datasets_for_training(train_filenames, val_filenames):\n",
        "    \"\"\"\n",
        "    prepare_secoora_datasets_for_training(train_filenames, val_filenames):\n",
        "    This funcion prepares train and validation datasets  by extracting features (images, bounding boxes, and class labels)\n",
        "    then map to preprocess_secoora_data, then apply prefetch, padded batch and label encoder\n",
        "    INPUTS:\n",
        "        * train_filenames [string]: tfrecord filenames for training\n",
        "        * val_filenames [string]: tfrecord filenames for validation\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * val_dataset [tensorflow dataset]: validation dataset\n",
        "        * train_dataset [tensorflow dataset]: training dataset\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "\n",
        "    features = {\n",
        "        'image': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "        'objects/xmin': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
        "        'objects/ymin': tf.io.FixedLenSequenceFeature([], tf.float32,allow_missing=True),\n",
        "        'objects/xmax': tf.io.FixedLenSequenceFeature([], tf.float32,allow_missing=True),\n",
        "        'objects/ymax': tf.io.FixedLenSequenceFeature([], tf.float32,allow_missing=True),\n",
        "        'objects/label': tf.io.FixedLenSequenceFeature([], tf.int64,allow_missing=True),\n",
        "    }\n",
        "\n",
        "\n",
        "    def _parse_function(example_proto):\n",
        "      # Parse the input `tf.train.Example` proto using the dictionary above.\n",
        "      return tf.io.parse_single_example(example_proto, features)\n",
        "\n",
        "    train_dataset = tf.data.TFRecordDataset(train_filenames)\n",
        "    train_dataset = train_dataset.map(_parse_function)\n",
        "\n",
        "    train_dataset = train_dataset.map(preprocess_secoora_data, num_parallel_calls=AUTO)\n",
        "\n",
        "    shapes = (tf.TensorShape([None,None,3]),tf.TensorShape([None,4]),tf.TensorShape([None,]))\n",
        "\n",
        "    # this is necessary because there are unequal numbers of labels in every image\n",
        "    train_dataset = train_dataset.padded_batch(\n",
        "        batch_size = BATCH_SIZE, drop_remainder=True, padding_values=(0.0, 1e-8, -1), padded_shapes=shapes,\n",
        "    )\n",
        "\n",
        "    label_encoder = LabelEncoderCoco()\n",
        "\n",
        "    # train_dataset = train_dataset.shuffle(8 * BATCH_SIZE)\n",
        "    train_dataset = train_dataset.map(\n",
        "        label_encoder.encode_batch, num_parallel_calls=AUTO\n",
        "    )\n",
        "\n",
        "    train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
        "    train_dataset = train_dataset.prefetch(AUTO)\n",
        "\n",
        "    val_dataset = tf.data.TFRecordDataset(val_filenames)\n",
        "    val_dataset = val_dataset.map(_parse_function)\n",
        "    val_dataset = val_dataset.map(preprocess_secoora_data, num_parallel_calls=AUTO)\n",
        "\n",
        "    val_dataset = val_dataset.padded_batch(\n",
        "        batch_size = BATCH_SIZE, padding_values=(0.0, 1e-8, -1), drop_remainder=True, padded_shapes=shapes,\n",
        "    )\n",
        "\n",
        "    val_dataset = val_dataset.map(\n",
        "        label_encoder.encode_batch, num_parallel_calls=AUTO\n",
        "    )\n",
        "    val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
        "    val_dataset = val_dataset.prefetch(AUTO)\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_secoora_data(example):\n",
        "    \"\"\"\n",
        "    preprocess_secoora_data(example)\n",
        "    \"\"\n",
        "    This function\n",
        "    INPUTS:\n",
        "        * val_dataset [tensorflow dataset]: validation dataset\n",
        "        * train_dataset [tensorflow dataset]: training dataset\n",
        "    OPTIONAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * val_dataset [tensorflow dataset]: validation dataset\n",
        "        * train_dataset [tensorflow dataset]: training dataset\n",
        "    GLOBAL INPUTS: None\n",
        "    \"\"\"\n",
        "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    #image = tf.image.per_image_standardization(image)\n",
        "\n",
        "    bbox = tf.numpy_function(np.array,[[example[\"objects/xmin\"], example[\"objects/ymin\"], example[\"objects/xmax\"], example[\"objects/ymax\"]]], tf.float32)\n",
        "    bbox = tf.transpose(bbox)\n",
        "\n",
        "    class_id = tf.cast(example[\"objects/label\"], dtype=tf.int32)\n",
        "\n",
        "    # image, bbox = random_flip_horizontal(image, bbox)\n",
        "    image, image_shape, ratio = resize_and_pad_image(image)\n",
        "\n",
        "    bbox3 = tf.stack(\n",
        "        [\n",
        "            bbox[:, 0] * ratio,\n",
        "            bbox[:, 1] * ratio,\n",
        "            bbox[:, 2] * ratio,\n",
        "            bbox[:, 3] * ratio,\n",
        "        ],\n",
        "        axis=-1,\n",
        "    )\n",
        "    bbox = convert_to_xywh(tf.cast(bbox3, tf.float32))\n",
        "\n",
        "    return image, bbox, class_id\n",
        "\n",
        "\n",
        "\n",
        "def resize_and_pad_image(\n",
        "    image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
        "):\n",
        "    \"\"\"\n",
        "    resize_and_pad_image(image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0)\n",
        "    Resizes and pads image while preserving aspect ratio.\n",
        "    1. Resizes images so that the shorter side is equal to `min_side`\n",
        "    2. If the longer side is greater than `max_side`, then resize the image\n",
        "      with longer side equal to `max_side`\n",
        "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
        "    `stride`\n",
        "    Arguments:\n",
        "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
        "        image.\n",
        "      min_side: The shorter side of the image is resized to this value, if\n",
        "        `jitter` is set to None.\n",
        "      max_side: If the longer side of the image exceeds this value after\n",
        "        resizing, the image is resized such that the longer side now equals to\n",
        "        this value.\n",
        "      jitter: A list of floats containing minimum and maximum size for scale\n",
        "        jittering. If available, the shorter side of the image will be\n",
        "        resized to a random value in this range.\n",
        "      stride: The stride of the smallest feature map in the feature pyramid.\n",
        "        Can be calculated using `image_size / feature_map_size`.\n",
        "    Returns:\n",
        "      image: Resized and padded image.\n",
        "      image_shape: Shape of the image before padding.\n",
        "      ratio: The scaling factor used to resize the image\n",
        "    \"\"\"\n",
        "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
        "    if jitter is not None:\n",
        "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
        "    ratio = min_side / tf.reduce_min(image_shape)\n",
        "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
        "        ratio = max_side / tf.reduce_max(image_shape)\n",
        "    image_shape = ratio * image_shape\n",
        "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
        "    padded_image_shape = tf.cast(\n",
        "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
        "    )\n",
        "    image = tf.image.pad_to_bounding_box(\n",
        "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
        "    )\n",
        "    return image, image_shape, ratio\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    \"\"\"\n",
        "    convert_to_xywh(boxes)\n",
        "    Changes the box format to center, width and height.\n",
        "    Arguments:\n",
        "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
        "        representing bounding boxes where each box is of the format\n",
        "        `[xmin, ymin, xmax, ymax]`.\n",
        "    Returns:\n",
        "      converted boxes with shape same as that of boxes.\n",
        "    \"\"\"\n",
        "    return tf.concat(\n",
        "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
        "        axis=-1,\n",
        "    )\n",
        "\n",
        "def int2str(class_dict, classes):\n",
        "    return [class_dict[i] for i in classes]\n",
        "\n",
        "def int2name(storms, item):\n",
        "    return storms[item]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZT40uAK9zIt"
      },
      "source": [
        "\n",
        "###############################################################\n",
        "## VARIABLES\n",
        "###############################################################\n",
        "\n",
        "patience = 5\n",
        "\n",
        "ims_per_shard = 20\n",
        "\n",
        "VALIDATION_SPLIT = 0.7 #6 #5\n",
        "\n",
        "# class_dict = {0: 'no-damage', 1: 'minor-damage', 2: 'major-damage', 3: 'destroyed'} #, 4: 'un-classified'}\n",
        "class_dict = {1:'building'}\n",
        "storms = ['matthew', 'michael', 'florence', 'harvey']\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "do_train =  True #False #True\n",
        "\n",
        "transfer_learn = False #True\n",
        "\n",
        "noise_stdev = 0.5 #0.2\n",
        "gamma = 4.0\n",
        "alpha = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "AymDSFrD94Td",
        "outputId": "98ac93ea-0f12-484e-8384-920ebf70cc2a"
      },
      "source": [
        "\n",
        "#==================================================================\n",
        "for storm in ['all']: #['harvey']: #, 'matthew', 'michael', 'florence', 'all']:\n",
        "\n",
        "    #storm = 'harvey' #'florence' #'michael' #'matthew'\n",
        "\n",
        "    if storm is 'all':\n",
        "        tfrecord_dir = []\n",
        "        sample_data_path = []; sample_label_data_path = []\n",
        "        for s in ['matthew', 'michael', 'florence', 'harvey']:\n",
        "            tfrecord_dir.append('MyDrive/xview2 dataset'+s+'/bbox' )\n",
        "            sample_data_path.append('MyDrive/xview2 dataset'+s)\n",
        "            sample_label_data_path.append('MyDrive/xview2 dataset'+s)\n",
        "    else:\n",
        "        tfrecord_dir = 'MyDrive/xview2 dataset'+storm+'/bbox'\n",
        "        sample_data_path = []; sample_label_data_path = []\n",
        "        for s in ['matthew', 'michael', 'florence', 'harvey']:\n",
        "            sample_data_path.append('MyDrive/xview2 dataset'+s)\n",
        "            sample_label_data_path.append('MyDrive/xview2 dataset'+s)\n",
        "\n",
        "    if transfer_learn is True:\n",
        "        weights = 'MyDrive/xview2 dataset'+storm+'/buildings-transfer_weights'\n",
        "    else:\n",
        "        weights = 'MyDrive/xview2 dataset'+storm+'/buildings-scratch_weights'\n",
        "\n",
        "    weights_path = 'MyDrive/xview2 dataset'+storm\n",
        "\n",
        "    if storm is 'all':\n",
        "        transfer_weights = os.path.join('MyDrive/xview2 dataset', \"buildings-scratch_weights\")\n",
        "    else:\n",
        "        transfer_weights = os.path.join('MyDrive/xview2 dataset'+storm+'/3_transfer_val0.6', \"buildings-transfer_weights\")\n",
        "        #transfer_weights = os.path.join('MyDrive/xview2 dataset'+storm+'/1_scratch_val0.5', \"buildings-scratch_weights\")\n",
        "\n",
        "\n",
        "    #============================================================\n",
        "    hist_fig = weights.replace('.h5','_history.png')\n",
        "\n",
        "    test_samples_fig = hist_fig.replace('_history.png','_samples.png')\n",
        "\n",
        "    lr_fig = weights.replace('.h5','_lr.png')\n",
        "\n",
        "    ###############################################################\n",
        "    ## EXECUTION\n",
        "    ###############################################################\n",
        "\n",
        "    #-------------------------------------------------\n",
        "    print('.....................................')\n",
        "    print('Reading files and making datasets ...')\n",
        "\n",
        "    if type(tfrecord_dir) is list:\n",
        "        filenames = []\n",
        "        for t in tfrecord_dir:\n",
        "            filenames.append(sorted(tf.io.gfile.glob(t+os.sep+'buildings-*.tfrec')))\n",
        "\n",
        "        filenames = np.hstack(filenames)\n",
        "\n",
        "    else:\n",
        "        filenames = sorted(tf.io.gfile.glob(tfrecord_dir+os.sep+'buildings-*.tfrec'))\n",
        "\n",
        "    shuffle(filenames)\n",
        "\n",
        "    nb_images = ims_per_shard * len(filenames)\n",
        "    print(nb_images)\n",
        "\n",
        "    split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "\n",
        "    training_filenames = filenames[split:]\n",
        "    validation_filenames = filenames[:split]\n",
        "\n",
        "    validation_steps = int(nb_images // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
        "    steps_per_epoch = int(nb_images // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
        "\n",
        "    print(steps_per_epoch)\n",
        "    print(validation_steps)\n",
        "\n",
        "    val_dataset, train_dataset = prepare_secoora_datasets_for_training(training_filenames, validation_filenames)\n",
        "\n",
        "    \"\"\"\n",
        "    ## LR\n",
        "    \"\"\"\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n",
        "\n",
        "    rng = [i for i in range(MAX_EPOCHS)]\n",
        "    y = [lrfn(x) for x in rng]\n",
        "    plt.plot(rng, [lrfn(x) for x in rng])\n",
        "    # plt.show()\n",
        "    if transfer_learn is True:\n",
        "        plt.savefig(os.getcwd()+os.sep+'results/learnratesched_transfer_bbox.png', dpi=200, bbox_inches='tight')\n",
        "    else:\n",
        "        plt.savefig(os.getcwd()+os.sep+'results/learnratesched_scratch_bbox.png', dpi=200, bbox_inches='tight')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".....................................\n",
            "Reading files and making datasets ...\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-42380233b403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mvalidation_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_images\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_images\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
          ]
        }
      ]
    }
  ]
}
